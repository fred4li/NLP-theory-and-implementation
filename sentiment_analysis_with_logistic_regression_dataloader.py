# -*- coding: utf-8 -*-
"""sentiment analysis with logistic regression_dataloader.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Rnvdb4FP5w21Pcja7NE6YYt_dnvu9Dz0
"""

# import related packages
#NLP library nltk: https://www.nltk.org/
import nltk 
import torch
import pandas as pd
import io
import numpy as np
from os import getcwd
from tqdm.notebook import tqdm

# download the textual data 
nltk.download('twitter_samples')

"""* stopwords: words like I, me, my, myself,...... that are considered neural in sentiment analysis. These words will be removed in the task of data preprocessing."""

# download stopwords
nltk.download('stopwords')

filePath = f"{getcwd()}/../tmp2/"
nltk.data.path.append(filePath)

filePath

import utils
from nltk.corpus import twitter_samples 
from utils import process_tweet, build_freqs, extract_features

"""# Exploratory analysis on data / Preprocess data"""

# select the set of positive and negative tweets
all_positive_tweets = twitter_samples.strings('positive_tweets.json')
all_negative_tweets = twitter_samples.strings('negative_tweets.json')

all_positive_tweets[:10]

df = pd.DataFrame(all_positive_tweets + all_negative_tweets, columns =['tweets'])
df.set_index('tweets', inplace=True)

df.head()

# label the tweets: 0 means negative, 1 means positive
df['label'] = [0] * df.shape[0]
df.loc[all_positive_tweets,'label'] = 1

df.head()

df.label.value_counts()

"""# Training/Validation Split"""

import sklearn
from sklearn.model_selection import train_test_split

X_train, X_val, y_train, y_val = train_test_split(
    df.index.values,
    df.label.values,   
    test_size = 0.15,  # 85% training, 15% testing/validating
    # random_state = 17, # for test purpose, guarantee comparabla results 
    # stratify = df.label.values (no need to stratify since the data is balanced)
)

# be careful about the shape of labels
y_train.shape

df['data_type'] = ['not_set'] * df.shape[0]

df.loc[X_train,'data_type'] = 'train'
df.loc[X_val,'data_type'] = 'val'

df.groupby(['label','data_type']).count()

df.head()

"""# Tokenizer: Convert text into numerical data


## Logistic regression and Naive Bayes


"""

freqs = build_freqs(X_train, y_train)
extract_features(X_train[0], freqs)

extract_features(X_train[0], freqs).shape

# use positive and negative frequency to tokenize input tweets
X_train_token = extract_features(X_train[0], freqs)
for i in range(1, len(X_train)):
  X_train_token = np.append(X_train_token, extract_features(X_train[i], freqs), axis = 0) 


X_val_token = extract_features(X_val[0], freqs)
for i in range(1, len(X_val)):
  X_val_token = np.append(X_val_token, extract_features(X_val[i], freqs), axis = 0)

X_val_token.shape

X_train_token.shape

y_train.shape

# y_train.resize((8500, 1))

# y_train.shape

#y_val

from torch.utils.data import TensorDataset

# convert np.array to pytorch tensor
X_train=torch.from_numpy(X_train_token.astype(np.float32))
X_val=torch.from_numpy(X_val_token.astype(np.float32))
y_train=torch.from_numpy(y_train.astype(np.float32))
y_val=torch.from_numpy(y_val.astype(np.float32))

X_train

y_train

# pair input features with labels
dataset_train = TensorDataset(X_train, y_train)
dataset_train

dataset_val = TensorDataset(X_val, y_val)
dataset_val

y_train.shape

X_train.shape

y_train=y_train.view(y_train.shape[0],1)
y_val=y_val.view(y_val.shape[0],1)
y_train.shape

"""# Setting up Model"""

class Logistic_Reg_model(torch.nn.Module):
 def __init__(self,no_input_features):
   super(Logistic_Reg_model,self).__init__()
   self.layer1=torch.nn.Linear(no_input_features,20)
   self.layer2=torch.nn.Linear(20,1)
 def forward(self,x):
   y_predicted=self.layer1(x)
   y_predicted=torch.sigmoid(self.layer2(y_predicted))
   return y_predicted

# initiate the model
model=Logistic_Reg_model(3)

"""# Creating dataloaders to iterate input data"""

from torch.utils.data import DataLoader, RandomSampler, SequentialSampler

batch_size = 8500

dataloader_train = DataLoader(dataset_train, 
                              sampler=RandomSampler(dataset_train), 
                              batch_size=batch_size,drop_last=True)

# explore data type
dataloader_train

"""## Another version below"""

batch_size = 100
n_iters = 3000
epochs = n_iters / (len(train_dataset) / batch_size)
input_dim = 3
output_dim = 1
lr_rate = 0.001
train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)

"""# Setting Up Optimizer and Scheduler"""

# Binary Cross Entropy Loss
criterion=torch.nn.BCELoss()
# Stochastic gradient descent
optimizer=torch.optim.SGD(model.parameters(),lr=0.01)
epochs = 10

batch_size = 8500
n_iters = 3000
epochs = 200
#epochs = int(n_iters / (len(dataloader_train) / batch_size))
for epoch in tqdm(range(1, epochs+1)):
    
    model.train()
    
    loss_train_total = 0

    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)
    for batch in progress_bar:

        model.zero_grad()


        batch = tuple(b for b in batch)
        
        # batch = tuple(b.to(device) for b in batch)

        outputs = model(batch[0])
        
        #inputs = {'input_ids':      batch[0],
        #          'attention_mask': batch[1],
        #          'labels':         batch[2],
        #         }       

        #outputs = model(**inputs)
        #batch[1] = batch[1].unsqueeze(1)
        loss=criterion(outputs,batch[1].resize(batch_size,1))
        
        loss_train_total += loss.item()
        loss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

        optimizer.step()
        #scheduler.step()
        
        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})
         
        
    #torch.save(model.state_dict(), f'finetuned_BERT_epoch_{epoch}.model')
        
    tqdm.write(f'\nEpoch {epoch}')
    
    loss_train_avg = loss_train_total/len(dataloader_train)            
    tqdm.write(f'Training loss: {loss_train_avg}')
    
    #val_loss, predictions, true_vals = evaluate(dataloader_validation)
    #val_f1 = f1_score_func(predictions, true_vals)
    #tqdm.write(f'Validation loss: {val_loss}')
    #tqdm.write(f'F1 Score (Weighted): {val_f1}')

with torch.no_grad():
 y_pred=model(X_val)
 y_pred_class=y_pred.round()
 accuracy=(y_pred_class.eq(y_val).sum())/float(y_val.shape[0])
 print(accuracy.item())

