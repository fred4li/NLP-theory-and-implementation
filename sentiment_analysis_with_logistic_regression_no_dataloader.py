# -*- coding: utf-8 -*-
"""sentiment analysis with logistic regression_no dataloader.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H-C26mY9y-5JgIcY3B9-sgSv4P6b7-7_
"""

# import the NLP library nltk: https://www.nltk.org/
import nltk 
import torch
import pandas as pd
import io
import numpy as np
from os import getcwd
from tqdm.notebook import tqdm

# download the textual data 
nltk.download('twitter_samples')

"""* stopwords: words like I, me, my, myself,...... that are considered neural in sentiment analysis. These words will be removed in the task of data preprocessing."""

# download stopwords
nltk.download('stopwords')

filePath = f"{getcwd()}/../tmp2/"
nltk.data.path.append(filePath)

filePath

import utils
from utils import process_tweet, build_freqs, extract_features
from nltk.corpus import twitter_samples

"""* import related packages and helper functions

# Exploratory analysis on data / Preprocess data
"""

# select the set of positive and negative tweets
all_positive_tweets = twitter_samples.strings('positive_tweets.json')
all_negative_tweets = twitter_samples.strings('negative_tweets.json')

# a sample of the first 10 positive tweets
all_positive_tweets[:10]

# form a panda dataframe to display all tweets
df = pd.DataFrame(all_positive_tweets + all_negative_tweets, columns =['tweets'])
df.set_index('tweets', inplace=True)

df.head()

# label the tweets: 0 means negative, 1 means positive
df['label'] = [0] * df.shape[0]
df.loc[all_positive_tweets,'label'] = 1

df.head()

df.label.value_counts()

"""# Training/Validation Split"""

# import sklearn
import sklearn
from sklearn.model_selection import train_test_split

X_train, X_val, y_train, y_val = train_test_split(
    df.index.values,
    df.label.values,   
    test_size = 0.15,  # 85% training, 15% testing/validating
    # random_state = 17, # for test purpose, guarantee comparabla results 
    # stratify = df.label.values (no need to stratify since the data is balanced)
)

# be careful about the shape of labels
y_train.shape

df['data_type'] = ['not_set'] * df.shape[0]

df.loc[X_train,'data_type'] = 'train'
df.loc[X_val,'data_type'] = 'val'

df.groupby(['label','data_type']).count()

df.head()

"""# Tokenizer: Convert text into numerical data


## Logistic regression and Naive Bayes


"""

freqs = build_freqs(X_train, y_train)

extract_features(X_train[0], freqs)

# np.concatenate((a, b), axis=None)

# use positive and negative frequency to tokenize input tweets
X_train_token = extract_features(X_train[0], freqs)
for i in range(1, len(X_train)):
  X_train_token = np.append(X_train_token, extract_features(X_train[i], freqs), axis = 0) 


X_val_token = extract_features(X_val[0], freqs)
for i in range(1, len(X_val)):
  X_val_token = np.append(X_val_token, extract_features(X_val[i], freqs), axis = 0)

X_val_token.shape

X_train_token.shape

y_train.shape

y_val

# scaler is a helper function to normalize input features
scaler=sklearn.preprocessing.StandardScaler()
X_train=scaler.fit_transform(X_train_token)
X_val=scaler.fit_transform(X_val_token)

# convert np.array to pytorch tensor
X_train=torch.from_numpy(X_train_token.astype(np.float32))
X_val=torch.from_numpy(X_val_token.astype(np.float32))
y_train=torch.from_numpy(y_train.astype(np.float32))
y_val=torch.from_numpy(y_val.astype(np.float32))

y_train.shape

X_train.shape

y_train=y_train.view(y_train.shape[0],1)
y_val=y_val.view(y_val.shape[0],1)

"""# Setting up Model"""

class Logistic_Reg_model(torch.nn.Module):
 def __init__(self,no_input_features):
   super(Logistic_Reg_model,self).__init__()
   self.layer1=torch.nn.Linear(no_input_features,20)
   self.layer2=torch.nn.Linear(20,1)
 def forward(self,x):
   y_predicted=self.layer1(x)
   y_predicted=torch.sigmoid(self.layer2(y_predicted))
   return y_predicted

# initiate the model
model=Logistic_Reg_model(3)

"""# creating dataloaders (not used in this version)"""

import torch
from torch.utils.data import TensorDataset
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler

dataloader_train = DataLoader(dataset_train, 
                              sampler=RandomSampler(dataset_train), 
                              batch_size=batch_size)

dataloader_train

"""# Setting Up Optimizer and Scheduler"""

# Binary Cross Entropy Loss
criterion=torch.nn.BCELoss()
# Stochastic gradient descent
optimizer=torch.optim.SGD(model.parameters(),lr=0.01)

number_of_epochs=3000
for epoch in range(number_of_epochs):
 y_prediction=model(X_train)
 loss=criterion(y_prediction,y_train)
 loss.backward()
 optimizer.step()
 optimizer.zero_grad()
 if (epoch+1)%100 == 0:
   print('epoch:', epoch+1,',loss=',loss.item())

# use the validation set to measure the accuracy of the model trained before
with torch.no_grad():
 y_pred=model(X_val)
 y_pred_class=y_pred.round()
 accuracy=(y_pred_class.eq(y_val).sum())/float(y_val.shape[0])
 print(accuracy.item())

